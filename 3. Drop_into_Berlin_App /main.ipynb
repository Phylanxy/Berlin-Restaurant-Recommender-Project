{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2f910e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2f910e",
        "outputId": "c0432ca4-11fb-4174-ccd6-6065e1cfdd65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (4.65.0)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (5.5.0)\n",
            "Collecting hdbscan>=0.8.29\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m374.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.3.5)\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.33)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->bertopic) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp39-cp39-linux_x86_64.whl size=3582071 sha256=24614bf371b358189f8fea762b99f44324a91c21e5ba164ea9be93f60a917eb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/6f/88/1a4c04276b98306f00217a1e300e6ba0252c6aa4f7616067ae\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=d248a33c404299357b85350d85b224e07d03eeefbc812dfa8f442f990e28a4c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=a0656c9d45e06dc1ee8c70186b9ebf699fda2725b38be71f7558dbd408273f22\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=52128316d083677645a428aabc5515da99b4395327b457285eb9cc5a1bc2057d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "Successfully installed bertopic-0.14.1 hdbscan-0.8.29 huggingface-hub-0.13.1 pynndescent-0.5.8 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uk4v_pZW8il2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk4v_pZW8il2",
        "outputId": "cbb6f163-db99-4490-dad0-44ff3ed4de2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=ef5c7e1442014e7fa96bfc2989d7da43eb46b952e19e25e53abb79edbeb6af23\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/7a/e9/22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, ftfy, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install clean-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pypKdWFv-hlk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pypKdWFv-hlk",
        "outputId": "aef14dd8-9e1b-4740-94ef-b0952931b40a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: clean-text in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from clean-text) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.9/dist-packages (from clean-text) (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n"
          ]
        }
      ],
      "source": [
        "pip install clean-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6FduP5L03PsT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FduP5L03PsT",
        "outputId": "65ec475c-8f22-456d-db2f-d972846e5932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement maps_search (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for maps_search\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install maps_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Mb4dTKq4eC1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mb4dTKq4eC1",
        "outputId": "742d1e02-a418-4cc4-87c5-c44a51017fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googlemaps\n",
            "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from googlemaps) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2.10)\n",
            "Building wheels for collected packages: googlemaps\n",
            "  Building wheel for googlemaps (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40715 sha256=927b17702e49fdeeab83f1465540b6c774c8bcb6bc31520fe2b56e298ab43d9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/5f/46/54a2bdb4bcb07d3faba4463d2884865705914cc72a7b8bb5f0\n",
            "Successfully built googlemaps\n",
            "Installing collected packages: googlemaps\n",
            "Successfully installed googlemaps-4.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U googlemaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RD556Zw-5aZL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD556Zw-5aZL",
        "outputId": "b1d9045a-f662-4ebd-9e17-9996a69d9bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gomaps\n",
            "  Downloading gomaps-0.3.3.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests_html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting pyppdf\n",
            "  Downloading pyppdf-0.1.2.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting GeoLiberator\n",
            "  Downloading GeoLiberator-0.3.7.tar.gz (9.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from pyppdf->gomaps) (2022.12.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from pyppdf->gomaps) (8.1.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from pyppdf->gomaps) (5.4.8)\n",
            "Collecting litereval>=0.0.9\n",
            "  Downloading litereval-0.0.11.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyppeteer>=0.2.2\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting w3lib\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from requests_html->gomaps) (2.25.1)\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.9/dist-packages (from requests_html->gomaps) (0.0.1)\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.2.2->pyppdf->gomaps) (1.26.14)\n",
            "Collecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting websockets<11.0,>=10.0\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.2.2->pyppdf->gomaps) (6.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from pyppeteer>=0.2.2->pyppdf->gomaps) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4->requests_html->gomaps) (4.6.3)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.9/dist-packages (from fake-useragent->requests_html->gomaps) (5.12.0)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.9/dist-packages (from pyquery->requests_html->gomaps) (4.9.2)\n",
            "Collecting cssselect>=1.2.0\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->requests_html->gomaps) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->requests_html->gomaps) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.2.2->pyppdf->gomaps) (3.15.0)\n",
            "Building wheels for collected packages: gomaps, GeoLiberator, pyppdf, litereval, parse\n",
            "  Building wheel for gomaps (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gomaps: filename=gomaps-0.3.3-py3-none-any.whl size=10551 sha256=801cd4c4496f99a3d9158e9adf1a663ea9754149d1813285468b8f4d1c588217\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e5/00/84f741959fb8059093ad2384874b4b03e0853d8557dfbff519\n",
            "  Building wheel for GeoLiberator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GeoLiberator: filename=GeoLiberator-0.3.7-py3-none-any.whl size=9611 sha256=dc9db6a12c3eee6c6a43ed6dbf1bae8d04a377e13852a218f9b99550ed1d5164\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/b6/19/1cd8adbf6ffa93512ab5a11e29122d6abbb60e49613f3d8343\n",
            "  Building wheel for pyppdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyppdf: filename=pyppdf-0.1.2-py3-none-any.whl size=11095 sha256=7284d32734df013b30b7968c34b9edb2382efec45f15c6bd086e60a20d58c2bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/41/3a/64c67deaedcbfc2b8caaeb2b7dd29dc448f992441a8b8982de\n",
            "  Building wheel for litereval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for litereval: filename=litereval-0.0.11-py3-none-any.whl size=6029 sha256=e4d61079959275e370f56d7699713e598659601b3c945632872b542c2508f932\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/71/d4/48431c0c6c44c4a2566e2c88c080c84550eefd322bb7e28b00\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=8ac9128cb80ed6dfc01c65914e1cabb2d6bf8fe4052b8b39006e52131e2efbcb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/9c/58/ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
            "Successfully built gomaps GeoLiberator pyppdf litereval parse\n",
            "Installing collected packages: pyee, parse, GeoLiberator, appdirs, websockets, w3lib, litereval, cssselect, pyquery, pyppeteer, fake-useragent, requests_html, pyppdf, gomaps\n",
            "Successfully installed GeoLiberator-0.3.7 appdirs-1.4.4 cssselect-1.2.0 fake-useragent-1.1.1 gomaps-0.3.3 litereval-0.0.11 parse-1.19.0 pyee-8.2.2 pyppdf-0.1.2 pyppeteer-1.0.2 pyquery-2.0.0 requests_html-0.10.0 w3lib-2.1.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "pip install gomaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DF_7nb0V5lJ4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF_7nb0V5lJ4",
        "outputId": "e7677fee-6ccc-418f-abb5-c87dc3bda20f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pgeocode\n",
            "  Downloading pgeocode-0.4.0-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pgeocode) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pgeocode) (2.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from pgeocode) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->pgeocode) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->pgeocode) (2022.7.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->pgeocode) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pgeocode) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pgeocode) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pgeocode) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->pgeocode) (1.15.0)\n",
            "Installing collected packages: pgeocode\n",
            "Successfully installed pgeocode-0.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pgeocode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NkC786cX5qbG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkC786cX5qbG",
        "outputId": "7f153a7e-d00a-4fa3-c83e-bb5e787e3de5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cleantext\n",
            "  Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from cleantext) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->cleantext) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->cleantext) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->cleantext) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->cleantext) (2022.6.2)\n",
            "Installing collected packages: cleantext\n",
            "Successfully installed cleantext-1.1.4\n"
          ]
        }
      ],
      "source": [
        "pip install cleantext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qDepRYqNMek0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "qDepRYqNMek0",
        "outputId": "da26211b-0a2c-471c-9e9a-bac9b3528f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.4.2-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Collecting ws4py\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45229 sha256=64a288132b3054f6268cb169db323c0266bbcf1a50d5c9a6898e4dd112e90ce6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/9d/fd/0a594a1bc8d493935d319ac40d64d79f5b4117ef4d37b1b678\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.4.2 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b381b6a",
      "metadata": {
        "id": "3b381b6a"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import googlemaps\n",
        "import json\n",
        "from gomaps import maps_search\n",
        "import time\n",
        "import pandas as pd\n",
        "import pgeocode\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from cleantext import clean\n",
        "from google.colab import drive\n",
        "import ast\n",
        "import locale\n",
        "import geopy.distance\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import anvil.server\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.vectorizers import ClassTfidfTransformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ln6JRusDMg99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln6JRusDMg99",
        "outputId": "f38066a5-d3b6-4ae9-ac07-ace50e85143a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "anvil.server.connect(\"server_IQCVHOJJA34NMQ2XRAXFKMLE-3D5ASWO7MRUDAWQI\")\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030043fb",
      "metadata": {
        "id": "030043fb"
      },
      "outputs": [],
      "source": [
        "database =  pd.read_csv(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/database.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea043a0",
      "metadata": {
        "id": "4ea043a0"
      },
      "outputs": [],
      "source": [
        "trial_model =  BERTopic.load(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/BERT_model_CPU.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jq6yWAlwrpt4",
      "metadata": {
        "id": "Jq6yWAlwrpt4"
      },
      "outputs": [],
      "source": [
        "api_key = open(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/key.txt\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r7Pv4JIf1gNL",
      "metadata": {
        "id": "r7Pv4JIf1gNL"
      },
      "outputs": [],
      "source": [
        "#Lukas Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ydVOSWmm2Qrw",
      "metadata": {
        "id": "ydVOSWmm2Qrw"
      },
      "outputs": [],
      "source": [
        "def extract_place_details(place_id, api_key):\n",
        "    url = f'https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&key={api_key}'\n",
        "\n",
        "    # Send a GET request to the API endpoint and retrieve the response\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the JSON response and convert it to a dictionary\n",
        "    result = json.loads(response.text)\n",
        "\n",
        "    try:\n",
        "        return [result[\"result\"]]\n",
        "    \n",
        "    except:\n",
        "        print(\"Error in extract_place_details\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5wwxMBTd2fNy",
      "metadata": {
        "id": "5wwxMBTd2fNy"
      },
      "outputs": [],
      "source": [
        "def transform_to_df(place_details):\n",
        "    user_input_details = pd.DataFrame(data = place_details)\n",
        "    return user_input_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UavL-IXr2Qzv",
      "metadata": {
        "id": "UavL-IXr2Qzv"
      },
      "outputs": [],
      "source": [
        "def process_user_input_details2(place_id, api_key):\n",
        "    #print(\"1. Please find your place_id of your favorite location using the following link: https://developers.google.com/maps/documentation/javascript/examples/places-placeid-finder\")\n",
        "    #place_id = input(\"Please insert the place_id of your desired location: \")\n",
        "    place_details = extract_place_details(place_id, api_key)\n",
        "    user_input_details = transform_to_df(place_details)\n",
        "    \n",
        "    return user_input_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1tPNsaK2dZ0p",
      "metadata": {
        "id": "1tPNsaK2dZ0p"
      },
      "outputs": [],
      "source": [
        "def get_reference_from_detais(user_input_details):\n",
        "  return user_input_details[\"reference\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDUB9Zr21vPk",
      "metadata": {
        "id": "hDUB9Zr21vPk"
      },
      "outputs": [],
      "source": [
        "def get_coordinates_from_details(user_input_details):\n",
        "    latitude_user_input = round(user_input_details[\"geometry\"][0][\"location\"][\"lat\"], 7)\n",
        "    longitude_user_input = round(user_input_details[\"geometry\"][0][\"location\"][\"lng\"], 7)\n",
        "    return latitude_user_input, longitude_user_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uj-kIu101vSL",
      "metadata": {
        "id": "uj-kIu101vSL"
      },
      "outputs": [],
      "source": [
        "def process_user_input_high_level(user_input_details, api_key):\n",
        "    latitude, longitude = get_coordinates_from_details(user_input_details)\n",
        "    \n",
        "    #Google Maps API parameters\n",
        "    search_string = user_input_details[\"name\"][0]\n",
        "    distance = 0.1\n",
        "    rankby=distance\n",
        "    \n",
        "    map_client = googlemaps.Client(api_key)\n",
        "    response = map_client.places_nearby(location=(latitude, longitude), keyword=search_string, radius=distance)\n",
        "    \n",
        "    results = response.get('results')\n",
        "\n",
        "    reference_id_user_input_detail = get_reference_from_detais(user_input_details)\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "      if results[i][\"reference\"] == reference_id_user_input_detail:\n",
        "        high_level_result = transform_to_df([result])\n",
        "\n",
        "        return high_level_result\n",
        "      else:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DoSEt6gA1vU-",
      "metadata": {
        "id": "DoSEt6gA1vU-"
      },
      "outputs": [],
      "source": [
        "def concat(df_details, df_high_level):\n",
        "    final = pd.concat([df_high_level.set_index(\"place_id\"), df_details.set_index(\"place_id\")], axis = 1, join=\"inner\")\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yHxBim1A1vXM",
      "metadata": {
        "id": "yHxBim1A1vXM"
      },
      "outputs": [],
      "source": [
        "def add_missing_cols(final_df, database):\n",
        "    benchmark_cols = database.columns\n",
        "    final_cols = final_df.columns\n",
        "    \n",
        "    for benchmark_column in benchmark_cols:\n",
        "        if benchmark_column not in final_cols:\n",
        "            final_df[benchmark_column] = np.nan\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XoK4QhX-1vZc",
      "metadata": {
        "id": "XoK4QhX-1vZc"
      },
      "outputs": [],
      "source": [
        "def clean_data(final_df):\n",
        "    \n",
        "    #Cleaning Step 1\n",
        "    final_df = final_df.loc[:, ~final_df.columns.duplicated(keep='first')]\n",
        "    \n",
        "    cols_to_drop = ['Unnamed: 0', 'utc_offset', 'secondary_opening_hours', 'current_opening_hours', 'adr_address', 'address_components', 'icon', 'icon_background_color', 'icon',\n",
        "       'icon_mask_base_uri', 'scope', 'formatted_phone_number', \"opening_hours\"]\n",
        "       \n",
        "    try:\n",
        "      final_df = final_df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    final_df = final_df[['name', 'reference', 'geometry', 'formatted_address',\n",
        "       'price_level', 'rating', 'user_ratings_total', 'types','editorial_summary', 'reviews',\n",
        "       'curbside_pickup', 'delivery', 'dine_in', 'takeout', 'reservable',\n",
        "       'serves_beer', 'serves_breakfast', 'serves_brunch', 'serves_dinner', 'serves_lunch', 'serves_vegetarian_food', 'serves_wine',\n",
        "       'wheelchair_accessible_entrance', 'website', 'international_phone_number', 'photos', 'vicinity', 'url', 'business_status','permanently_closed', 'plus_code']]\n",
        "    \n",
        "    #Cleaning Step 2\n",
        "    review_dct = {}\n",
        "    for i, item in enumerate(final_df[\"reviews\"][0]):\n",
        "        review_dct[f\"review{i}\"] = item[\"text\"]\n",
        "        #TBD: to be double checked\n",
        "        #review_dct[f\"review{i}\"] = clean(item[\"text\"], no_emoji=True, no_line_breaks=True)\n",
        "    try:\n",
        "        review_dct.update({\"editorial_summary\" : ast.literal_eval(final_df[\"editorial_summary\"][0])[\"overview\"]})\n",
        "        \n",
        "    except:\n",
        "        review_dct.update({\"editorial_summary\" : \"\"})\n",
        "\n",
        "    try:\n",
        "        review_dct.update({\"types\" : ast.literal_eval(final_df[\"types\"][0].replace(\"'point_of_interest', \", \"\"))})\n",
        "        \n",
        "    except:\n",
        "        review_dct.update({\"types\" : \"\"})\n",
        "    \n",
        "    #Cleaning Step 3\n",
        "    final_df[\"model_input\"] = str(review_dct).replace(\"{\", \"\").replace(\"}\",\"\")\n",
        "    final_df[\"model_input\"].apply(lambda x: str(x).replace(\"'review0':\" , \"\").replace(\"'review1':\" , \"\").replace(\"'review2':\" , \"\").replace(\"'review3':\" , \"\").replace(\"'review4':\" , \"\").replace(\"'types':\" , \"\").replace(\"'editorial_summary':\" , \"\"))  \n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PuVK63fMRCNm",
      "metadata": {
        "id": "PuVK63fMRCNm"
      },
      "outputs": [],
      "source": [
        "def user_input(place_id, api_key, database):\n",
        "    user_input_details = process_user_input_details2(place_id, api_key)\n",
        "    user_input_high_level = process_user_input_high_level(user_input_details, api_key)\n",
        "    final = concat(user_input_details, user_input_high_level)\n",
        "    final = add_missing_cols(final, database)\n",
        "    final_cleaned = clean_data(final)\n",
        "    \n",
        "    return final_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n2bnwu7JPjry",
      "metadata": {
        "id": "n2bnwu7JPjry"
      },
      "outputs": [],
      "source": [
        "def calculate_distance(user_input_details, variable):\n",
        "  user_input_location = get_coordinates_from_details(user_input_details)\n",
        "  database_cell_latitude = ast.literal_eval(variable)[\"location\"][\"lat\"]\n",
        "  database_cell_longitude = ast.literal_eval(variable)[\"location\"][\"lng\"]\n",
        "  return round(geopy.distance.great_circle(user_input_location, (database_cell_latitude, database_cell_longitude)).km,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DmOG44Xp9fZI",
      "metadata": {
        "id": "DmOG44Xp9fZI"
      },
      "outputs": [],
      "source": [
        "def add_distance(user_input_details, database):\n",
        "  database[\"distance\"] = database[\"geometry\"].apply(lambda x: calculate_distance(user_input_details, x))\n",
        "  return database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uBxzXocd1gnO",
      "metadata": {
        "id": "uBxzXocd1gnO"
      },
      "outputs": [],
      "source": [
        "#Leo Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc4b036",
      "metadata": {
        "id": "1bc4b036"
      },
      "outputs": [],
      "source": [
        "def find_topic_of_database(trial_model, database):\n",
        "    database[\"topic\"] = trial_model.topics_\n",
        "    #TBD: enhance dataframe with tags\n",
        "    return database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd91c725",
      "metadata": {
        "id": "dd91c725"
      },
      "outputs": [],
      "source": [
        "def find_topic_of_user_input(model, user_input):\n",
        "    if type(user_input) == str:\n",
        "        #print(\"str\")\n",
        "        input_topic, input_prob = model.transform(user_input)\n",
        "        user_input_type = \"Manual Input\"\n",
        "    elif type(user_input) != \"None\":\n",
        "        #print(\"df\")\n",
        "        input_topic, input_prob = model.transform(str(user_input[\"model_input\"]))\n",
        "        user_input_type = \"Google Maps Input\"\n",
        "        #name = user_input[\"name\"].to_string(index=False)\n",
        "    else:\n",
        "        print(\"error while assigning the topic for user input\")\n",
        "    \n",
        "    return user_input_type, input_topic, input_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b71e541",
      "metadata": {
        "id": "5b71e541"
      },
      "outputs": [],
      "source": [
        "def build_clusters(model, database, user_input):\n",
        "    #cluster building of database\n",
        "    database_clustered = find_topic_of_database(model, database)\n",
        "    user_input_type, user_input_topic, user_input_prob = find_topic_of_user_input(model, user_input)\n",
        "    return database_clustered, user_input_type, user_input_topic, user_input_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lRLiDE6INUvY",
      "metadata": {
        "id": "lRLiDE6INUvY"
      },
      "outputs": [],
      "source": [
        "#Für Leo wenn er fertig ist\n",
        "def output_recommendation(database, user_input_type, user_input_topic, user_input_prob, hidden_yes_no, sort_by_rating, sort_by_distance, pricetag):\n",
        "  # remove user input from possible list of output if it's a restaurant\n",
        "  if user_input_type == \"Manual Input\":\n",
        "    #TBD: to be implemented\n",
        "    recommendation = database.iloc[random.choice(range(len(database[database[\"topic\"] == user_input_topic[0]])))][\"name\"]\n",
        "    return recommendation\n",
        "  \n",
        "  \n",
        "  elif user_input_type == \"Google Maps Input\":\n",
        "    \n",
        "    recommendation_df = database[(database[\"topic\"] == user_input_topic[0]) & (database[\"price_level\"] == pricetag) & (database[\"business_status\"] == \"OPERATIONAL\") & (database[\"hidden\"] == hidden_yes_no)]\n",
        "    \n",
        "    if sort_by_rating == True and sort_by_distance == True:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by=['rating', 'distance'], ascending=[False, True])[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\", \"hidden\"]][0:19]\n",
        "    elif sort_by_rating == True and sort_by_distance == False:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by='rating', ascending=False)[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\", \"hidden\"]][0:19]\n",
        "    elif sort_by_rating == False and sort_by_distance == True:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by='distance', ascending=True)[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\", \"hidden\"]][0:19]\n",
        "    else:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by=['rating', 'distance'], ascending=[False, True])[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\", \"hidden\"]][0:19]\n",
        "    \n",
        "    recommendation_final = recommendation_df_sorted.to_dict(orient='records')\n",
        "\n",
        "    return recommendation_final\n",
        "    \n",
        "  else:\n",
        "    return \"error while selecting the recommendation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6-yjp0aWDeA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "_6-yjp0aWDeA",
        "outputId": "9384292c-6ccf-45bf-fa95-d8cd00746d09"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def output_recommendation(database, user_input_type, user_input_topic, user_input_prob, sort_by_rating, sort_by_distance, pricetag):\\n  # remove user input from possible list of output if it\\'s a restaurant\\n  if user_input_type == \"Manual Input\":\\n    #TBD: to be implemented\\n    recommendation = database.iloc[random.choice(range(len(database[database[\"topic\"] == user_input_topic[0]])))][\"name\"]\\n    return recommendation\\n  \\n  \\n  elif user_input_type == \"Google Maps Input\":\\n    \\n    recommendation_df = database[(database[\"topic\"] == user_input_topic[0]) & (database[\"price_level\"] == pricetag) & (database[\"business_status\"] == \"OPERATIONAL\")]\\n    \\n    if sort_by_rating == True and sort_by_distance == True:\\n      recommendation_df_sorted = recommendation_df.sort_values(by=[\\'rating\\', \\'distance\\'], ascending=[False, True])[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\\n    elif sort_by_rating == True and sort_by_distance == False:\\n      recommendation_df_sorted = recommendation_df.sort_values(by=\\'rating\\', ascending=False)[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\\n    elif sort_by_rating == False and sort_by_distance == True:\\n      recommendation_df_sorted = recommendation_df.sort_values(by=\\'distance\\', ascending=True)[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\\n    else:\\n      recommendation_df_sorted = recommendation_df.sort_values(by=[\\'rating\\', \\'distance\\'], ascending=[False, True])[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\\n    \\n    recommendation_final = recommendation_df_sorted.to_dict(orient=\\'records\\')\\n\\n    return recommendation_final\\n    \\n  else:\\n    return \"error while selecting the recommendation\"'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''def output_recommendation(database, user_input_type, user_input_topic, user_input_prob, sort_by_rating, sort_by_distance, pricetag):\n",
        "  # remove user input from possible list of output if it's a restaurant\n",
        "  if user_input_type == \"Manual Input\":\n",
        "    #TBD: to be implemented\n",
        "    recommendation = database.iloc[random.choice(range(len(database[database[\"topic\"] == user_input_topic[0]])))][\"name\"]\n",
        "    return recommendation\n",
        "  \n",
        "  \n",
        "  elif user_input_type == \"Google Maps Input\":\n",
        "    \n",
        "    recommendation_df = database[(database[\"topic\"] == user_input_topic[0]) & (database[\"price_level\"] == pricetag) & (database[\"business_status\"] == \"OPERATIONAL\")]\n",
        "    \n",
        "    if sort_by_rating == True and sort_by_distance == True:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by=['rating', 'distance'], ascending=[False, True])[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\n",
        "    elif sort_by_rating == True and sort_by_distance == False:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by='rating', ascending=False)[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\n",
        "    elif sort_by_rating == False and sort_by_distance == True:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by='distance', ascending=True)[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\n",
        "    else:\n",
        "      recommendation_df_sorted = recommendation_df.sort_values(by=['rating', 'distance'], ascending=[False, True])[[\"name\", \"website\", \"price_level\", \"rating\", \"distance\", \"photos\", \"geometry\"]][0:19]\n",
        "    \n",
        "    recommendation_final = recommendation_df_sorted.to_dict(orient='records')\n",
        "\n",
        "    return recommendation_final\n",
        "    \n",
        "  else:\n",
        "    return \"error while selecting the recommendation\"'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ryy5Gu1C1lk3",
      "metadata": {
        "id": "Ryy5Gu1C1lk3"
      },
      "outputs": [],
      "source": [
        "#Combining Lukas and Leos Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "829e40f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "829e40f4",
        "outputId": "2d8b9303-309d-4fd5-97b4-50a779f18551"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def main(database, model, api_key, place_id, sort_by_rating, sort_by_distance, pricetag):\\n    user_input_final = user_input(place_id, api_key, database)\\n    database_incl_distance = add_distance(user_input_final, database)\\n    database_clustered, user_input_type, user_input_topic, user_input_prob = build_clusters(model, database_incl_distance, user_input_final)\\n    recommendation = output_recommendation(database_clustered, user_input_type, user_input_topic, user_input_prob, sort_by_rating, sort_by_distance, pricetag)\\n    return recommendation'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''def main(database, model, api_key, place_id, sort_by_rating, sort_by_distance, pricetag):\n",
        "    user_input_final = user_input(place_id, api_key, database)\n",
        "    database_incl_distance = add_distance(user_input_final, database)\n",
        "    database_clustered, user_input_type, user_input_topic, user_input_prob = build_clusters(model, database_incl_distance, user_input_final)\n",
        "    recommendation = output_recommendation(database_clustered, user_input_type, user_input_topic, user_input_prob, sort_by_rating, sort_by_distance, pricetag)\n",
        "    return recommendation'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DudjaRYNrA2l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "DudjaRYNrA2l",
        "outputId": "02c35d2d-10c7-43b7-c959-f1537d069454"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#Übergangsfunktion\\n@anvil.server.callable\\ndef main(place_id, description, sort_by_rating, sort_by_distance, pricetag):\\n    database =  pd.read_csv(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/Archive/df_with_model_input.csv\")\\n    trial_model =  BERTopic.load(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/Archive/BERT_model_custom_embeddings_v1.joblib\")\\n    api_key = open(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/key.txt\").read()\\n    \\n    user_input_final = user_input(place_id, api_key, database)\\n    database_incl_distance = add_distance(user_input_final, database)\\n    database_clustered, user_input_type, user_input_topic, user_input_prob = build_clusters(trial_model, database_incl_distance, user_input_final)\\n    recommendation = output_recommendation(database_clustered, user_input_type, user_input_topic, user_input_prob, sort_by_rating, sort_by_distance, pricetag)\\n    return recommendation'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''#Übergangsfunktion\n",
        "@anvil.server.callable\n",
        "def main(place_id, description, sort_by_rating, sort_by_distance, pricetag):\n",
        "    database =  pd.read_csv(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/Archive/df_with_model_input.csv\")\n",
        "    trial_model =  BERTopic.load(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/Archive/BERT_model_custom_embeddings_v1.joblib\")\n",
        "    api_key = open(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/key.txt\").read()\n",
        "    \n",
        "    user_input_final = user_input(place_id, api_key, database)\n",
        "    database_incl_distance = add_distance(user_input_final, database)\n",
        "    database_clustered, user_input_type, user_input_topic, user_input_prob = build_clusters(trial_model, database_incl_distance, user_input_final)\n",
        "    recommendation = output_recommendation(database_clustered, user_input_type, user_input_topic, user_input_prob, sort_by_rating, sort_by_distance, pricetag)\n",
        "    return recommendation'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "muyal4lzBqav",
      "metadata": {
        "id": "muyal4lzBqav"
      },
      "outputs": [],
      "source": [
        "#Final\n",
        "@anvil.server.callable\n",
        "def main(place_id, hidden_yes_no, sort_by_rating, sort_by_distance, pricetag):\n",
        "    database =  pd.read_csv(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/fina_database.csv\")\n",
        "    trial_model =  BERTopic.load(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/BERT_model_MiniLM_eom_test_CPU.joblib\")\n",
        "    api_key = open(\"/content/drive/MyDrive/Final Project Ironhack Drop into Berin/key.txt\").read()\n",
        "    \n",
        "    user_input_final = user_input(place_id, api_key, database)\n",
        "    database_incl_distance = add_distance(user_input_final, database)\n",
        "    database_clustered, user_input_type, user_input_topic, user_input_prob = build_clusters(trial_model, database_incl_distance, user_input_final)\n",
        "    recommendation = output_recommendation(database_clustered, user_input_type, user_input_topic, user_input_prob, hidden_yes_no, sort_by_rating, sort_by_distance, pricetag)\n",
        "    return recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2vfKStt9zP66",
      "metadata": {
        "id": "2vfKStt9zP66"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
